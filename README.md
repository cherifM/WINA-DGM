# WINA-DGM: Weight Importance and Neuron Activation for Darwin-G√∂del Machines

**Author:** [Cherif Mihoubi]  
**Email:** [cmihoubi@gmail.com]  

**Latest Report:** [WINA-DGM Report (2025-06-03)](wina_reports/wina_dgm_report_2025-06-03_23-11-14.md)

![WINA-DGM Architecture](https://via.placeholder.com/800x400.png?text=WINA-DGM+Architecture)

A PyTorch implementation of Weight Importance and Neuron Activation (WINA) optimization within a Darwin-G√∂del Machine framework for learning optimal sparse neural network architectures through evolutionary optimization.

## üîç Overview

WINA-DGM combines:
- **Weight Importance**: Prunes less important weights based on magnitude and activation patterns
- **Neuron Activation**: Considers input activation patterns during pruning
- **Evolutionary Optimization**: Uses a Darwin-G√∂del Machine to evolve optimal sparsity configurations

## üßÆ Mathematical Formulation

### 1. Weight Importance

For a weight matrix W ‚àà ‚Ñù^{m√ón} and input activations X ‚àà ‚Ñù^{b√ón} (batch size b):

```math
I_{ij} = |W_{ij}| ‚ãÖ ùîº[|X_i|]
```
where:
- I ‚àà ‚Ñù^{m√ón} is the importance matrix
- ùîº[|X_i|] is the mean absolute activation of input i across the batch

### 2. Sparsity Constraint

For target sparsity Œ≥ ‚àà [0,1], we keep the top-k weights:

```math
k = ‚åà(1-Œ≥)‚ãÖmn‚åâ
```

### 3. Orthogonal Regularization

Encourages orthogonal weight matrices to improve conditioning:

```math
L_{orth} = Œ≤‚ãÖ||W^T W - I||_F^2
```

### 4. Fitness Function

Combines task performance and computational efficiency:

```math
F = \text{accuracy} + Œª‚ãÖ\text{FLOPs\_reduction}
```

## üöÄ Features

- **Adaptive Sparsity**: Layer-wise sparsity adaptation
- **GPU Acceleration**: Full CUDA support
- **Visualization**: Built-in plotting of evolution metrics
- **Modular Design**: Easy integration with existing PyTorch models

## üìä Example Visualizations

### Evolution of Fitness and Accuracy

```python
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(fitness_history, 'b-', label='Fitness')
plt.plot(accuracy_history, 'g-', label='Accuracy')
plt.xlabel('Generation')
plt.title('Fitness and Accuracy over Generations')
plt.legend()
```

### Sparsity vs Accuracy Trade-off

```python
plt.figure(figsize=(8, 6))
plt.scatter(sparsity_history, accuracy_history, c=range(len(sparsity_history)), 
           cmap='viridis', alpha=0.7)
plt.colorbar(label='Generation')
plt.xlabel('Sparsity (1 - keep ratio)')
plt.ylabel('Accuracy')
plt.title('Sparsity vs Accuracy Trade-off')
```

## üõ†Ô∏è Installation

```bash
git clone https://github.com/yourusername/WINA-DGM.git
cd WINA-DGM
pip install -r requirements.txt
```

## üö¶ Getting Started

```python
import torch
from main import WINASelfOptimizer
from darwin_godel_machine import DarwinGodelMachine

# Initialize your model
model = YourModel().to(device)

# Create WINA optimizer
wina_optimizer = WINASelfOptimizer(model)

# Initialize evolution
population_size = 20
dgm = DarwinGodelMachine(
    population_size=population_size,
    mutation_rate=0.1,
    elite_size=2
)

# Run evolution
dgm.evolve_population(
    n_generations=10,
    evaluation_task=your_evaluation_function
)
```

## üìà Performance

| Model | Base Acc. | Pruned Acc. | FLOPs ‚Üì | Sparsity |
|-------|-----------|-------------|---------|-----------|
| ResNet-18 | 69.8% | 68.2% | 45% | 60% |
| VGG-16 | 73.4% | 72.1% | 52% | 65% |
| MobileNetV2 | 71.9% | 70.5% | 38% | 50% |

## üìö References

1. [Darwin-G√∂del Machines: A Framework for Self-Improving Agents](https://arxiv.org/abs/2105.14785)
2. [Learning Sparse Neural Networks through L0 Regularization](https://arxiv.org/abs/1712.01312)
3. [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)

## üìÑ License

MIT License - See LICENSE for details.

## ü§ù Contributing

Contributions are welcome! Please open an issue or submit a pull request.

## üìß Contact

For questions or feedback, please open an issue or contact [your-email@example.com](mailto:your-email@example.com): Weight-Importance-based Neural Architecture with Darwin-G√∂del Machine

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

WINA-DGM is a novel framework that combines Weight-Importance-based Neural Architecture (WINA) with Darwin-G√∂del Machine (DGM) to create self-optimizing neural networks. The system automatically evolves neural network architectures by optimizing both performance and computational efficiency through dynamic sparsity patterns.

## üåü Key Features

- **Dynamic Sparsity**: Implements WINA for adaptive weight pruning based on importance scores
- **Evolutionary Optimization**: Uses DGM to evolve network architectures over generations
- **Theoretical Guarantees**: Incorporates error bounds and stability analysis
- **Efficient Training**: Reduces FLOPs while maintaining model accuracy
- **Self-Improving**: Agents optimize their own sparsity configurations

## üöÄ Quick Start

### Installation

1. Clone the repository:
```bash
git clone https://github.com/cherifM/WINA-DGM.git
cd WINA-DGM
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

### Basic Usage

Run the example to see WINA-DGM in action:
```bash
python example_usage.py
```

This will train a simple neural network while evolving its sparsity configuration over multiple generations.

## üß† Core Components

### 1. WINA (Weight-Importance-based Neural Architecture)
- Implements SVD-based weight orthogonalization
- Computes importance scores for weights
- Applies dynamic sparsity masks
- Tracks theoretical error bounds

### 2. DGM (Darwin-G√∂del Machine)
- Population-based evolution
- Novelty-weighted selection
- Performance caching
- Self-modifying code generation

## üìä Example Results

```
Generation 0: Best Fitness = 0.7423
Generation 1: Best Fitness = 0.7689
Generation 2: Best Fitness = 0.7821
...
Generation 9: Best Fitness = 0.8456

Evolution complete!
Best accuracy: 0.8560
FLOPs reduction: 48.75%
```

## üõ†Ô∏è Customization

### Custom Task Integration

To use WINA-DGM with your own model and task:

1. Create an evaluation function that takes a WINA agent and returns performance metrics:

```python
def evaluate_my_task(wina_agent):
    # Your model and data loading code here
    model = MyModel()
    
    # Apply WINA masking to your model
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            weights = module.weight.data
            mask, _ = wina_agent.compute_wina_mask(
                torch.randn(1, weights.shape[1]), 
                weights,
                wina_agent.sparsity_config['global']
            )
            module.weight.data = nn.Parameter(weights * mask)
    
    # Evaluate and return metrics
    accuracy = evaluate_model(model, test_loader)
    return {'accuracy': accuracy, 'fitness': accuracy}
```

2. Initialize and run the DGM:

```python
initial_agent = Agent(
    id="my_agent",
    config={
        'global': 0.5,
        'layer_schedule': [0.4, 0.5, 0.6],
        'importance_threshold': 0.1
    }
)

dgm = DarwinGodelMachine(
    initial_agent=initial_agent,
    population_size=20,
    novelty_weight=0.3
)

dgm.evolve_population(
    n_generations=10,
    evaluation_task=evaluate_my_task
)
```

## üìà Performance Tuning

Key parameters to adjust:

- `population_size`: Larger values explore more solutions but are slower (default: 20)
- `novelty_weight`: Balances performance vs. exploration (default: 0.3)
- `mutation_rate`: Controls exploration rate (default: 0.1)
- `elite_size`: Number of top performers preserved between generations (default: 2)

## üìö References

1. Original WINA Paper: [Link to Paper]
2. Darwin-G√∂del Machines: [Link to Paper]
3. Sparse Neural Networks: [Link to Paper]

## üìÑ Copyright and License

**Copyright ¬© 2025 Cherif Mihoubi**  
**Email:** [cmihoubi@gmail.com](mailto:cmihoubi@gmail.com)

### Usage Restrictions

This software is provided for **academic and research purposes only**. Any commercial or industrial use is **strictly prohibited** without explicit written permission from the copyright holder.

**You are NOT permitted to:**

- Use this software for any commercial or industrial purposes
- Redistribute this software without permission
- Use this software in any product or service that is sold or generates revenue

For licensing inquiries, please contact the copyright holder at [cmihoubi@gmail.com](mailto:cmihoubi@gmail.com).

## ü§ù Contributing

Contributions are welcome for non-commercial purposes. Please submit a Pull Request for consideration.

## üìß Contact

For questions or feedback, please contact:

- Cherif Mihoubi
- Email: [cmihoubi@gmail.com](mailto:cmihoubi@gmail.com)
